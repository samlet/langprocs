{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:24:16.610803Z",
     "start_time": "2018-11-06T12:24:16.265994Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyhanlp import HanLP, JClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:24:22.301115Z",
     "start_time": "2018-11-06T12:24:18.740973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t他\t他\tr\tr\t_\t5\t主谓关系\t_\t_\n",
      "2\t在\t在\tp\tp\t_\t5\t状中结构\t_\t_\n",
      "3\t浙江\t浙江\tns\tns\t_\t4\t定中关系\t_\t_\n",
      "4\t金华\t金华\tnh\tnr\t_\t2\t介宾关系\t_\t_\n",
      "5\t出生\t出生\tv\tv\t_\t0\t核心关系\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence=\"他在浙江金华出生\"\n",
    "conll = HanLP.parseDependency(sentence).__str__()\n",
    "print(conll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T15:05:03.346056Z",
     "start_time": "2018-11-04T15:05:03.332464Z"
    }
   },
   "outputs": [],
   "source": [
    "def describe_rel(word):\n",
    "    if word.DEPREL==\"主谓关系\":\n",
    "        print(\"\\tactor: {}\".format(word.LEMMA))\n",
    "    elif word.DEPREL==\"动宾关系\":\n",
    "        print(\"\\tobject: {}\".format(word.LEMMA))\n",
    "    elif word.DEPREL==\"标点符号\":\n",
    "        pass\n",
    "    else:    \n",
    "        print(\"\\trel.{}({}): {}\".format(word.POSTAG, word.DEPREL, word.LEMMA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T15:05:18.883472Z",
     "start_time": "2018-11-04T15:05:18.771815Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core: vn - 通\n",
      "\tactor: 普京\n",
      "\trel.b(状中结构): 特朗普\n",
      "\tobject: 电话\n",
      "\trel.v(并列关系): 讨论\n",
      "总统/n 普京/nr 与/c 特朗普/nr 通/vn 电话/n 讨论/v 美国/ns 太空/s 探索/vn 技术/n 公司/n 。/w\n",
      "** 总统普京与特朗普通电话讨论美国太空探索技术公司。\n"
     ]
    }
   ],
   "source": [
    "sentence=\"总统普京与特朗普通电话讨论美国太空探索技术公司。\"\n",
    "conll = HanLP.parseDependency(sentence)\n",
    "coreindex=0\n",
    "for word in conll.iterator():\n",
    "    if word.HEAD==CoNLLWord.ROOT:\n",
    "        coreindex=word.ID\n",
    "        print(\"core: {} - {}\".format(word.POSTAG, word.LEMMA))\n",
    "for word in conll.iterator():\n",
    "    if word.HEAD.ID==coreindex:\n",
    "        describe_rel(word)\n",
    "\n",
    "print(NLPTokenizer.analyze(sentence))\n",
    "print(\"** \"+sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T14:58:16.316524Z",
     "start_time": "2018-11-04T14:58:16.305670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[我/r, 新/d, 造/v, 一个/m, 词/n, 叫/v, 幻想乡/ns, 你/r, 能/v, 识别/v, 并/c, 正确/ad, 标注/v, 词性/n, 吗/y, ？/w]\n",
      "我/代词 的/助词 希望/名词 是/动词 希望/动词 张晚霞/人名 的/助词 背影/名词 被/介词 晚霞/名词 映红/人名\n",
      "支援/v 臺灣/ns 正體/n 香港/ns 繁體/n ：/v [微软/nt 公司/n]/nt 於/p 1975年/t 由/p 比爾·蓋茲/n 和/c 保羅·艾倫/nr 創立/v 。/w\n"
     ]
    }
   ],
   "source": [
    "NLPTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "print(NLPTokenizer.segment(\"我新造一个词叫幻想乡你能识别并正确标注词性吗？\"))  # “正确”是副形词。\n",
    "# 注意观察下面两个“希望”的词性、两个“晚霞”的词性\n",
    "print(NLPTokenizer.analyze(\"我的希望是希望张晚霞的背影被晚霞映红\").translateLabels())\n",
    "print(NLPTokenizer.analyze(\"支援臺灣正體香港繁體：微软公司於1975年由比爾·蓋茲和保羅·艾倫創立。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T15:00:41.196617Z",
     "start_time": "2018-11-04T15:00:41.188275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[总统/n, 普京/nr, 与/c, 特朗普/b, 通/vn, 电话/n, 讨论/v, 美国/ns, 太空/s, 探索/vn, 技术/n, 公司/n, 。/w]\n",
      "总统/n 普京/nr 与/c 特朗普/nr 通/vn 电话/n 讨论/v 美国/ns 太空/s 探索/vn 技术/n 公司/n 。/w\n"
     ]
    }
   ],
   "source": [
    "print(NLPTokenizer.segment(\"总统普京与特朗普通电话讨论美国太空探索技术公司。\"))\n",
    "print(NLPTokenizer.analyze(\"总统普京与特朗普通电话讨论美国太空探索技术公司。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T14:07:24.137627Z",
     "start_time": "2018-11-04T14:07:24.040266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "徐先生 --(主谓关系)--> 帮助\n",
      "2\n",
      "还 --(状中结构)--> 帮助\n",
      "3\n",
      "具体 --(状中结构)--> 帮助\n",
      "4\n",
      "帮助 --(核心关系)--> ##核心##\n",
      "5\n",
      "他 --(兼语)--> 帮助\n",
      "6\n",
      "确定 --(动宾关系)--> 帮助\n",
      "7\n",
      "了 --(右附加关系)--> 确定\n",
      "8\n",
      "把 --(状中结构)--> 作为\n",
      "9\n",
      "画 --(介宾关系)--> 把\n",
      "10\n",
      "雄鹰 --(动宾关系)--> 画\n",
      "11\n",
      "、 --(标点符号)--> 松鼠\n",
      "12\n",
      "松鼠 --(并列关系)--> 雄鹰\n",
      "13\n",
      "和 --(左附加关系)--> 麻雀\n",
      "14\n",
      "麻雀 --(并列关系)--> 雄鹰\n",
      "15\n",
      "作为 --(动宾关系)--> 确定\n",
      "16\n",
      "主攻 --(定中关系)--> 目标\n",
      "17\n",
      "目标 --(动宾关系)--> 作为\n",
      "18\n",
      "。 --(标点符号)--> 帮助\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\")\n",
    "for word in sentence.iterator():  # 通过dir()可以查看sentence的方法\n",
    "    print(word.ID)\n",
    "    print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T14:10:36.524529Z",
     "start_time": "2018-11-04T14:10:36.498758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "徐先生 --(主谓关系)--> 帮助\n",
      "还 --(状中结构)--> 帮助\n",
      "具体 --(状中结构)--> 帮助\n",
      "帮助 --(核心关系)--> ##核心##\n",
      "他 --(兼语)--> 帮助\n",
      "确定 --(动宾关系)--> 帮助\n",
      "了 --(右附加关系)--> 确定\n",
      "把 --(状中结构)--> 作为\n",
      "画 --(介宾关系)--> 把\n",
      "雄鹰 --(动宾关系)--> 画\n",
      "、 --(标点符号)--> 松鼠\n",
      "松鼠 --(并列关系)--> 雄鹰\n",
      "和 --(左附加关系)--> 麻雀\n",
      "麻雀 --(并列关系)--> 雄鹰\n",
      "作为 --(动宾关系)--> 确定\n",
      "主攻 --(定中关系)--> 目标\n",
      "目标 --(动宾关系)--> 作为\n",
      "。 --(标点符号)--> 帮助\n",
      "\n",
      "麻雀 --(并列关系)--> \n",
      "雄鹰 --(动宾关系)--> \n",
      "画 --(介宾关系)--> \n",
      "把 --(状中结构)--> \n",
      "作为 --(动宾关系)--> \n",
      "确定 --(动宾关系)--> \n",
      "帮助 --(核心关系)--> \n",
      "##核心##\n"
     ]
    }
   ],
   "source": [
    "# 也可以直接拿到数组，任意顺序或逆序遍历\n",
    "word_array = sentence.getWordArray()\n",
    "for word in word_array:\n",
    "    print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "print()\n",
    "\n",
    "# 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根\n",
    "CoNLLWord = JClass(\"com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord\")\n",
    "head = word_array[12]\n",
    "while head.HEAD:\n",
    "    head = head.HEAD\n",
    "    if (head == CoNLLWord.ROOT):\n",
    "        print(head.LEMMA)\n",
    "    else:\n",
    "        print(\"%s --(%s)--> \" % (head.LEMMA, head.DEPREL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:27:00.505008Z",
     "start_time": "2018-11-06T12:27:00.493233Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyhanlp import *\n",
    "\n",
    "\n",
    "def demo_custom_dictionary(text):\n",
    "    \"\"\" 演示用户词典的动态增删\n",
    "    TO-DO:\n",
    "    DoubleArrayTrie分词\n",
    "    首字哈希之后二分的trie树分词\n",
    "\n",
    "    >>> text = \"攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰\"  # 怎么可能噗哈哈！\n",
    "    >>> demo_custom_dictionary(text)\n",
    "    [攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
    "    [攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nz, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
    "    \"\"\"\n",
    "    print(HanLP.segment(text))\n",
    "\n",
    "    CustomDictionary = JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "    CustomDictionary.add(\"攻城狮\")  # 动态增加\n",
    "    CustomDictionary.insert(\"白富美\", \"nz 1024\")  # 强行插入\n",
    "    #CustomDictionary.remove(\"攻城狮\"); # 删除词语（注释掉试试）\n",
    "    CustomDictionary.add(\"单身狗\", \"nz 1024 n 1\")\n",
    "    #print(CustomDictionary.get(\"单身狗\"))\n",
    "\n",
    "    print(HanLP.segment(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:28:01.865436Z",
     "start_time": "2018-11-06T12:28:01.559175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
      "[攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nz, ，/w, 走上/v, 人生/n, 巅峰/n]\n"
     ]
    }
   ],
   "source": [
    "text = \"攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰\"  # 怎么可能噗哈哈！\n",
    "demo_custom_dictionary(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:36:44.486220Z",
     "start_time": "2018-11-06T12:36:44.450962Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_custom_nature():\n",
    "    \"\"\" 演示自定义词性,以及往词典中插入自定义词性的词语\n",
    "        !!!由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!!!\n",
    "\n",
    "    TO-DO\n",
    "    如果使用了动态词性之后任何类使用了switch(nature)语句,必须注册每个类\n",
    "\n",
    "    >>> demo_custom_nature()\n",
    "    n\n",
    "    None\n",
    "    电脑品牌\n",
    "    [苹果电脑/电脑品牌, 可以/v, 运行/vn, 开源/v, 阿尔法/nrf, 狗/n, 代码/n, 吗/y]\n",
    "    找到了 [电脑品牌] : 苹果电脑\n",
    "    <BLANKLINE>\n",
    "    [苹果电脑/电脑品牌, 可以/v, 运行/vn, 开源/v, 阿尔法狗/科技名词, 代码/n, 吗/y]\n",
    "    \"\"\"\n",
    "    # 对于系统中已有的词性,可以直接获取\n",
    "    Nature = JClass(\"com.hankcs.hanlp.corpus.tag.Nature\")\n",
    "    pc_nature = Nature.fromString(\"n\")\n",
    "    print(pc_nature)\n",
    "    # 此时系统中没有\"电脑品牌\"这个词性\n",
    "    pc_nature = Nature.fromString(\"电脑品牌\")\n",
    "    print(pc_nature)\n",
    "    # 我们可以动态添加一个\n",
    "    pc_nature = Nature.create(\"电脑品牌\");\n",
    "    print(pc_nature)\n",
    "    # 可以将它赋予到某个词语\n",
    "    LexiconUtility = JClass(\"com.hankcs.hanlp.utility.LexiconUtility\")\n",
    "    LexiconUtility.setAttribute(\"苹果电脑\", pc_nature)\n",
    "    # 或者\n",
    "    LexiconUtility.setAttribute(\"苹果电脑\", \"电脑品牌 1000\")\n",
    "    # 它们将在分词结果中生效\n",
    "    term_list = HanLP.segment(\"苹果电脑可以运行开源阿尔法狗代码吗\")\n",
    "    print(term_list)\n",
    "    for term in term_list:\n",
    "        if term.nature == pc_nature:\n",
    "            print(\"找到了 [{}] : {}\\n\".format(pc_nature, term.word))\n",
    "\n",
    "    # 还可以直接插入到用户词典\n",
    "    CustomDictionary = JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "    CustomDictionary.insert(\"阿尔法狗\", \"科技名词 1024\")\n",
    "    StandardTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.StandardTokenizer\")\n",
    "    StandardTokenizer.SEGMENT.enablePartOfSpeechTagging(True)  # 依然支持隐马词性标注\n",
    "    term_list = HanLP.segment(\"苹果电脑可以运行开源阿尔法狗代码吗\")\n",
    "    print(term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:36:58.160995Z",
     "start_time": "2018-11-06T12:36:58.108504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "None\n",
      "电脑品牌\n",
      "[苹果电脑/电脑品牌, 可以/v, 运行/vn, 开源/v, 阿尔法/nrf, 狗/n, 代码/n, 吗/y]\n",
      "找到了 [电脑品牌] : 苹果电脑\n",
      "\n",
      "[苹果电脑/电脑品牌, 可以/v, 运行/vn, 开源/v, 阿尔法狗/科技名词, 代码/n, 吗/y]\n"
     ]
    }
   ],
   "source": [
    "demo_custom_nature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:37:56.463700Z",
     "start_time": "2018-11-06T12:37:56.435248Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_dependency_parser():\n",
    "    \"\"\" 依存句法分析（CRF句法模型需要-Xms512m -Xmx512m -Xmn256m，\n",
    "        MaxEnt和神经网络句法模型需要-Xms1g -Xmx1g -Xmn512m）\n",
    "\n",
    "    >>> demo_dependency_parser()\n",
    "    徐先生 --(主谓关系)--> 帮助\n",
    "    还 --(状中结构)--> 帮助\n",
    "    具体 --(状中结构)--> 帮助\n",
    "    帮助 --(核心关系)--> ##核心##\n",
    "    他 --(兼语)--> 帮助\n",
    "    确定 --(动宾关系)--> 帮助\n",
    "    了 --(右附加关系)--> 确定\n",
    "    把 --(状中结构)--> 作为\n",
    "    画 --(介宾关系)--> 把\n",
    "    雄鹰 --(动宾关系)--> 画\n",
    "    、 --(标点符号)--> 松鼠\n",
    "    松鼠 --(并列关系)--> 雄鹰\n",
    "    和 --(左附加关系)--> 麻雀\n",
    "    麻雀 --(并列关系)--> 雄鹰\n",
    "    作为 --(动宾关系)--> 确定\n",
    "    主攻 --(定中关系)--> 目标\n",
    "    目标 --(动宾关系)--> 作为\n",
    "    。 --(标点符号)--> 帮助\n",
    "    <BLANKLINE>\n",
    "    徐先生 --(主谓关系)--> 帮助\n",
    "    还 --(状中结构)--> 帮助\n",
    "    具体 --(状中结构)--> 帮助\n",
    "    帮助 --(核心关系)--> ##核心##\n",
    "    他 --(兼语)--> 帮助\n",
    "    确定 --(动宾关系)--> 帮助\n",
    "    了 --(右附加关系)--> 确定\n",
    "    把 --(状中结构)--> 作为\n",
    "    画 --(介宾关系)--> 把\n",
    "    雄鹰 --(动宾关系)--> 画\n",
    "    、 --(标点符号)--> 松鼠\n",
    "    松鼠 --(并列关系)--> 雄鹰\n",
    "    和 --(左附加关系)--> 麻雀\n",
    "    麻雀 --(并列关系)--> 雄鹰\n",
    "    作为 --(动宾关系)--> 确定\n",
    "    主攻 --(定中关系)--> 目标\n",
    "    目标 --(动宾关系)--> 作为\n",
    "    。 --(标点符号)--> 帮助\n",
    "    <BLANKLINE>\n",
    "    麻雀 --(并列关系)-->\n",
    "    雄鹰 --(动宾关系)-->\n",
    "    画 --(介宾关系)-->\n",
    "    把 --(状中结构)-->\n",
    "    作为 --(动宾关系)-->\n",
    "    确定 --(动宾关系)-->\n",
    "    帮助 --(核心关系)-->\n",
    "    ##核心##\n",
    "    \"\"\"\n",
    "    sentence = HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\")\n",
    "    for word in sentence.iterator():  # 通过dir()可以查看sentence的方法\n",
    "        print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "    print()\n",
    "\n",
    "    # 也可以直接拿到数组，任意顺序或逆序遍历\n",
    "    word_array = sentence.getWordArray()\n",
    "    for word in word_array:\n",
    "        print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "    print()\n",
    "\n",
    "    # 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根\n",
    "    CoNLLWord = JClass(\"com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord\")\n",
    "    head = word_array[12]\n",
    "    while head.HEAD:\n",
    "        head = head.HEAD\n",
    "        if (head == CoNLLWord.ROOT):\n",
    "            print(head.LEMMA)\n",
    "        else:\n",
    "            print(\"%s --(%s)--> \" % (head.LEMMA, head.DEPREL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:38:13.623846Z",
     "start_time": "2018-11-06T12:38:13.509420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "徐先生 --(主谓关系)--> 帮助\n",
      "还 --(状中结构)--> 帮助\n",
      "具体 --(状中结构)--> 帮助\n",
      "帮助 --(核心关系)--> ##核心##\n",
      "他 --(兼语)--> 帮助\n",
      "确定 --(动宾关系)--> 帮助\n",
      "了 --(右附加关系)--> 确定\n",
      "把 --(状中结构)--> 作为\n",
      "画 --(介宾关系)--> 把\n",
      "雄鹰 --(动宾关系)--> 画\n",
      "、 --(标点符号)--> 松鼠\n",
      "松鼠 --(并列关系)--> 雄鹰\n",
      "和 --(左附加关系)--> 麻雀\n",
      "麻雀 --(并列关系)--> 雄鹰\n",
      "作为 --(动宾关系)--> 确定\n",
      "主攻 --(定中关系)--> 目标\n",
      "目标 --(动宾关系)--> 作为\n",
      "。 --(标点符号)--> 帮助\n",
      "\n",
      "徐先生 --(主谓关系)--> 帮助\n",
      "还 --(状中结构)--> 帮助\n",
      "具体 --(状中结构)--> 帮助\n",
      "帮助 --(核心关系)--> ##核心##\n",
      "他 --(兼语)--> 帮助\n",
      "确定 --(动宾关系)--> 帮助\n",
      "了 --(右附加关系)--> 确定\n",
      "把 --(状中结构)--> 作为\n",
      "画 --(介宾关系)--> 把\n",
      "雄鹰 --(动宾关系)--> 画\n",
      "、 --(标点符号)--> 松鼠\n",
      "松鼠 --(并列关系)--> 雄鹰\n",
      "和 --(左附加关系)--> 麻雀\n",
      "麻雀 --(并列关系)--> 雄鹰\n",
      "作为 --(动宾关系)--> 确定\n",
      "主攻 --(定中关系)--> 目标\n",
      "目标 --(动宾关系)--> 作为\n",
      "。 --(标点符号)--> 帮助\n",
      "\n",
      "麻雀 --(并列关系)--> \n",
      "雄鹰 --(动宾关系)--> \n",
      "画 --(介宾关系)--> \n",
      "把 --(状中结构)--> \n",
      "作为 --(动宾关系)--> \n",
      "确定 --(动宾关系)--> \n",
      "帮助 --(核心关系)--> \n",
      "##核心##\n"
     ]
    }
   ],
   "source": [
    "demo_dependency_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:39:03.923648Z",
     "start_time": "2018-11-06T12:39:03.911689Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def demo_high_speed_segment():\n",
    "    \"\"\" 演示极速分词，基于DoubleArrayTrie实现的词典正向最长分词，适用于“高吞吐量”“精度一般”的场合\n",
    "\n",
    "    >>> demo_high_speed_segment()\n",
    "    [江西, 鄱阳湖, 干枯, ，, 中国, 最大, 淡水湖, 变成, 大草原]\n",
    "    SpeedTokenizer分词速度：1253607.32字每秒\n",
    "    \"\"\"\n",
    "    SpeedTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.SpeedTokenizer\")\n",
    "    text = \"江西鄱阳湖干枯，中国最大淡水湖变成大草原\"\n",
    "    JClass(\"com.hankcs.hanlp.HanLP$Config\").ShowTermNature = False\n",
    "    print(SpeedTokenizer.segment(text))\n",
    "\n",
    "    start = time.time()\n",
    "    pressure = 1000000\n",
    "    for i in range(pressure):\n",
    "        SpeedTokenizer.segment(text)\n",
    "    cost_time = time.time() - start\n",
    "    print(\"SpeedTokenizer分词速度：%.2f字每秒\" % (len(text) * pressure / cost_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:39:23.354354Z",
     "start_time": "2018-11-06T12:39:07.076031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[江西, 鄱阳湖, 干枯, ，, 中国, 最大, 淡水湖, 变成, 大草原]\n",
      "SpeedTokenizer分词速度：1228942.75字每秒\n"
     ]
    }
   ],
   "source": [
    "demo_high_speed_segment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:40:36.476820Z",
     "start_time": "2018-11-06T12:40:36.471001Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_keyword(content):\n",
    "    \"\"\" 关键词提取\n",
    "\n",
    "    >>> content = (\n",
    "    ...    \"程序员(英文Programmer)是从事程序开发、维护的专业人员。\"\n",
    "    ...    \"一般将程序员分为程序设计人员和程序编码人员，\"\n",
    "    ...    \"但两者的界限并不非常清楚，特别是在中国。\"\n",
    "    ...    \"软件从业人员分为初级程序员、高级程序员、系统\"\n",
    "    ...    \"分析员和项目经理四大类。\")\n",
    "    >>> demo_keyword(content)\n",
    "    [程序员, 程序, 分为, 人员, 软件]\n",
    "    \"\"\"\n",
    "    TextRankKeyword = JClass(\"com.hankcs.hanlp.summary.TextRankKeyword\")\n",
    "    keyword_list = HanLP.extractKeyword(content, 5)\n",
    "    print(keyword_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:40:58.768012Z",
     "start_time": "2018-11-06T12:40:58.654147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[程序员, 人员, 程序, 分为, 开发]\n"
     ]
    }
   ],
   "source": [
    "content = (\n",
    "   \"程序员(英文Programmer)是从事程序开发、维护的专业人员。\"\n",
    "   \"一般将程序员分为程序设计人员和程序编码人员，\"\n",
    "   \"但两者的界限并不非常清楚，特别是在中国。\"\n",
    "   \"软件从业人员分为初级程序员、高级程序员、系统\"\n",
    "   \"分析员和项目经理四大类。\")\n",
    "demo_keyword(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:42:22.561884Z",
     "start_time": "2018-11-06T12:42:22.554726Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_NLP_segment():\n",
    "    \"\"\" NLP分词，更精准的中文分词、词性标注与命名实体识别\n",
    "        标注集请查阅 https://github.com/hankcs/HanLP/blob/master/data/dictionary/other/TagPKU98.csv\n",
    "        或者干脆调用 Sentence#translateLabels() 转为中文\n",
    "\n",
    "    >>> demo_NLP_segment()\n",
    "    [我/r, 新造/v, 一个/m, 词/n, 叫/v, 幻想乡/ns, 你/r, 能/v, 识别/v, 并/c, 正确/ad, 标注/v, 词性/n, 吗/y, ？/w]\n",
    "    我/代词 的/助词 希望/名词 是/动词 希望/动词 张晚霞/人名 的/助词 背影/名词 被/介词 晚霞/名词 映/动词 红/形容词\n",
    "    支援/v 臺灣/ns 正體/n 香港/ns 繁體/n ：/w [微软/nt 公司/n]/nt 於/p 1975年/t 由/p 比爾·蓋茲/n 和/c 保羅·艾倫/nr 創立/v 。/w\n",
    "    \"\"\"\n",
    "    NLPTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "    print(NLPTokenizer.segment(\"我新造一个词叫幻想乡你能识别并正确标注词性吗？\"))  # “正确”是副形词。\n",
    "    # 注意观察下面两个“希望”的词性、两个“晚霞”的词性\n",
    "    print(NLPTokenizer.analyze(\"我的希望是希望张晚霞的背影被晚霞映红\").translateLabels())\n",
    "    print(NLPTokenizer.analyze(\"支援臺灣正體香港繁體：微软公司於1975年由比爾·蓋茲和保羅·艾倫創立。\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:42:25.021308Z",
     "start_time": "2018-11-06T12:42:24.993494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[我, 新, 造, 一个, 词, 叫, 幻想乡, 你, 能, 识别, 并, 正确, 标注, 词性, 吗, ？]\n",
      "我/代词 的/助词 希望/名词 是/动词 希望/动词 张晚霞/人名 的/助词 背影/名词 被/介词 晚霞/名词 映红/人名\n",
      "支援/v 臺灣/ns 正體/n 香港/ns 繁體/n ：/v [微软/nt 公司/n]/nt 於/p 1975年/t 由/p 比爾·蓋茲/n 和/c 保羅·艾倫/nr 創立/v 。/w\n"
     ]
    }
   ],
   "source": [
    "demo_NLP_segment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:42:56.539390Z",
     "start_time": "2018-11-06T12:42:56.529615Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_normalization():\n",
    "    \"\"\" 演示正规化字符配置项的效果（繁体->简体，全角->半角，大写->小写）。\n",
    "        该配置项位于hanlp.properties中，通过Normalization=true来开启\n",
    "        切换配置后必须删除CustomDictionary.txt.bin缓存，否则只影响动态插入的新词。\n",
    "\n",
    "    >>> demo_normalization()\n",
    "    [爱听4g/nz]\n",
    "    [爱听4g/nz]\n",
    "    [爱听4g/nz]\n",
    "    [爱听4g/nz]\n",
    "    [爱听4g/nz]\n",
    "    \"\"\"\n",
    "    CustomDictionary =JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "    Config = JClass(\"com.hankcs.hanlp.HanLP$Config\")\n",
    "\n",
    "    Config.Normalization = True\n",
    "    CustomDictionary.insert(\"爱听4G\", \"nz 1000\")\n",
    "    print(HanLP.segment(\"爱听4g\"))\n",
    "    print(HanLP.segment(\"爱听4G\"))\n",
    "    print(HanLP.segment(\"爱听４G\"))\n",
    "    print(HanLP.segment(\"爱听４Ｇ\"))\n",
    "    print(HanLP.segment(\"愛聽４Ｇ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:43:06.139907Z",
     "start_time": "2018-11-06T12:43:06.130279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[爱听4g]\n",
      "[爱听4g]\n",
      "[爱听4g]\n",
      "[爱听4g]\n",
      "[爱听4g]\n"
     ]
    }
   ],
   "source": [
    "demo_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:48:07.681144Z",
     "start_time": "2018-11-06T12:48:07.672507Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_number_and_quantifier_recognition(sentences):\n",
    "    \"\"\" 演示数词和数量词识别\n",
    "\n",
    "    >>> sentences = [\n",
    "    ...    \"十九元套餐包括什么\",\n",
    "    ...    \"九千九百九十九朵玫瑰\",\n",
    "    ...    \"壹佰块都不给我\",\n",
    "    ...    \"９０１２３４５６７８只蚂蚁\",\n",
    "    ...    \"牛奶三〇〇克*2\",\n",
    "    ...    \"ChinaJoy“扫黄”细则露胸超2厘米罚款\",\n",
    "    ... ]\n",
    "    >>> demo_number_and_quantifier_recognition(sentences)\n",
    "    [十九元/mq, 套餐/n, 包括/v, 什么/ry]\n",
    "    [九千九百九十九朵/mq, 玫瑰/n]\n",
    "    [壹佰块/mq, 都/d, 不/d, 给/p, 我/rr]\n",
    "    [９０１２３４５６７８只/mq, 蚂蚁/n]\n",
    "    [牛奶/nf, 三〇〇克/mq, */w, 2/m]\n",
    "    [ChinaJoy/nx, “/w, 扫黄/vi, ”/w, 细则/n, 露/v, 胸/ng, 超/v, 2厘米/mq, 罚款/vi]\n",
    "    \"\"\"\n",
    "    StandardTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.StandardTokenizer\")\n",
    "\n",
    "    StandardTokenizer.SEGMENT.enableNumberQuantifierRecognize(True)\n",
    "    for sentence in sentences:\n",
    "        print(StandardTokenizer.segment(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:48:10.092120Z",
     "start_time": "2018-11-06T12:48:10.081534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[十九元, 套餐, 包括, 什么]\n",
      "[九千九百九十九朵, 玫瑰]\n",
      "[壹佰块, 都, 不, 给, 我]\n",
      "[9012345678只, 蚂蚁]\n",
      "[牛奶, 三〇〇克, *, 2]\n",
      "[chinajoy, \", 扫黄, \", 细则, 露, 胸, 超, 2厘米, 罚款]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "   \"十九元套餐包括什么\",\n",
    "   \"九千九百九十九朵玫瑰\",\n",
    "   \"壹佰块都不给我\",\n",
    "   \"９０１２３４５６７８只蚂蚁\",\n",
    "   \"牛奶三〇〇克*2\",\n",
    "   \"ChinaJoy“扫黄”细则露胸超2厘米罚款\",\n",
    "]\n",
    "demo_number_and_quantifier_recognition(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:49:53.059356Z",
     "start_time": "2018-11-06T12:49:52.958093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[我, 在, 上海, 林原科技有限公司, 兼职, 工作, ,]\n",
      "[我, 经常, 在, 台川喜宴餐厅, 吃饭, ,]\n",
      "[偶尔, 去, 开元地中海影城, 看, 电影, 。]\n"
     ]
    }
   ],
   "source": [
    "def demo_organization_recognition(sentences):\n",
    "    \"\"\" 机构名识别\n",
    "\n",
    "    >>> sentences = [\n",
    "    ...    \"我在上海林原科技有限公司兼职工作，\",\n",
    "    ...    \"我经常在台川喜宴餐厅吃饭，\",\n",
    "    ...    \"偶尔去开元地中海影城看电影。\",\n",
    "    ... ]\n",
    "    >>> demo_organization_recognition(sentences)\n",
    "    [我/rr, 在/p, 上海/ns, 林原科技有限公司/nt, 兼职/vn, 工作/vn, ，/w]\n",
    "    [我/rr, 经常/d, 在/p, 台川喜宴餐厅/nt, 吃饭/vi, ，/w]\n",
    "    [偶尔/d, 去/vf, 开元地中海影城/nt, 看/v, 电影/n, 。/w]\n",
    "    \"\"\"\n",
    "    Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "    Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "\n",
    "    segment = HanLP.newSegment().enableOrganizationRecognize(True)\n",
    "    for sentence in sentences:\n",
    "        term_list = segment.seg(sentence)\n",
    "        print(term_list)\n",
    "\n",
    "sentences = [\n",
    "   \"我在上海林原科技有限公司兼职工作，\",\n",
    "   \"我经常在台川喜宴餐厅吃饭，\",\n",
    "   \"偶尔去开元地中海影城看电影。\",\n",
    "]\n",
    "demo_organization_recognition(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:56:51.409470Z",
     "start_time": "2018-11-06T12:56:51.392184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未标注： [教授, 正在, 教授, 自然语言处理, 课程]\n",
      "标注后： [教授, 正在, 教授, 自然语言处理, 课程]\n"
     ]
    }
   ],
   "source": [
    "def demo_pos_tagging():\n",
    "    \"\"\" 词性标注\n",
    "\n",
    "    >>> demo_pos_tagging()\n",
    "    未标注： [教授/nnt, 正在/d, 教授/nnt, 自然语言处理/nz, 课程/n]\n",
    "    标注后： [教授/nnt, 正在/d, 教授/v, 自然语言处理/nz, 课程/n]\n",
    "    \"\"\"\n",
    "    Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "    text = \"教授正在教授自然语言处理课程\"\n",
    "    segment = HanLP.newSegment()\n",
    "\n",
    "    print(\"未标注：\", segment.seg(text))\n",
    "    segment.enablePartOfSpeechTagging(True)\n",
    "    print(\"标注后：\", segment.seg(text))\n",
    "\n",
    "demo_pos_tagging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:16:31.841450Z",
     "start_time": "2018-11-06T13:16:31.816914Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "from pyhanlp.static import download, remove_file, HANLP_DATA_PATH\n",
    "\n",
    "\n",
    "def test_data_path():\n",
    "    \"\"\"\n",
    "    获取测试数据路径，位于$root/data/test，根目录由配置文件指定。\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(HANLP_DATA_PATH, 'test')\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def ensure_data(data_name, data_url):\n",
    "    root_path = test_data_path()\n",
    "    dest_path = os.path.join(root_path, data_name)\n",
    "    if os.path.exists(dest_path):\n",
    "        return dest_path\n",
    "    if data_url.endswith('.zip'):\n",
    "        dest_path += '.zip'\n",
    "    download(data_url, dest_path)\n",
    "    if data_url.endswith('.zip'):\n",
    "        with zipfile.ZipFile(dest_path, \"r\") as archive:\n",
    "            archive.extractall(root_path)\n",
    "        remove_file(dest_path)\n",
    "        dest_path = dest_path[:-len('.zip')]\n",
    "    return dest_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:16:43.642357Z",
     "start_time": "2018-11-06T13:16:35.850700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://hanlp.linrunsoft.com/release/corpus/sogou-text-classification-corpus-mini.zip to /Users/xiaofeiwu/miniconda3/envs/bigdata/lib/python3.6/site-packages/pyhanlp/static/data/test/搜狗文本分类语料库迷你版.zip\n",
      "100.00%, 9 MB, 2052 KB/s, ETA 0 min 0 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pyhanlp import SafeJClass\n",
    "# from tests.test_utility import ensure_data\n",
    "\n",
    "NaiveBayesClassifier = SafeJClass('com.hankcs.hanlp.classification.classifiers.NaiveBayesClassifier')\n",
    "IOUtil = SafeJClass('com.hankcs.hanlp.corpus.io.IOUtil')\n",
    "sogou_corpus_path = ensure_data('搜狗文本分类语料库迷你版',\n",
    "                                'http://hanlp.linrunsoft.com/release/corpus/sogou-text-classification-corpus-mini.zip')\n",
    "\n",
    "\n",
    "def train_or_load_classifier():\n",
    "    model_path = sogou_corpus_path + '.ser'\n",
    "    if os.path.isfile(model_path):\n",
    "        return NaiveBayesClassifier(IOUtil.readObjectFrom(model_path))\n",
    "    classifier = NaiveBayesClassifier()\n",
    "    classifier.train(sogou_corpus_path)\n",
    "    model = classifier.getModel()\n",
    "    IOUtil.saveObjectTo(model, model_path)\n",
    "    return NaiveBayesClassifier(model)\n",
    "\n",
    "\n",
    "def predict(classifier, text):\n",
    "    print(\"《%16s》\\t属于分类\\t【%s】\" % (text, classifier.classify(text)))\n",
    "    # 如需获取离散型随机变量的分布，请使用predict接口\n",
    "    # print(\"《%16s》\\t属于分类\\t【%s】\" % (text, classifier.predict(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:19:05.090613Z",
     "start_time": "2018-11-06T13:19:02.849777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《C罗压梅西内马尔蝉联金球奖 2017=C罗年》\t属于分类\t【汽车】\n",
      "《英国造航母耗时8年仍未服役 被中国速度远远甩在身后》\t属于分类\t【军事】\n",
      "《 研究生考录模式亟待进一步专业化》\t属于分类\t【教育】\n",
      "《如果真想用食物解压,建议可以食用燕麦》\t属于分类\t【健康】\n",
      "《通用及其部分竞争对手目前正在考虑解决库存问题》\t属于分类\t【汽车】\n"
     ]
    }
   ],
   "source": [
    "classifier = train_or_load_classifier()\n",
    "predict(classifier, \"C罗压梅西内马尔蝉联金球奖 2017=C罗年\")\n",
    "predict(classifier, \"英国造航母耗时8年仍未服役 被中国速度远远甩在身后\")\n",
    "predict(classifier, \"研究生考录模式亟待进一步专业化\")\n",
    "predict(classifier, \"如果真想用食物解压,建议可以食用燕麦\")\n",
    "predict(classifier, \"通用及其部分竞争对手目前正在考虑解决库存问题\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:52:45.571357Z",
     "start_time": "2018-11-06T13:52:45.322237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文， 重载不是重任！\n",
      "拼音（数字音调）， [chong2, zai3, bu2, shi4, zhong4, ren4, none5]\n",
      "拼音（符号音调）， chóng, zǎi, bú, shì, zhòng, rèn, none, \n",
      "拼音（无音调）， chong, zai, bu, shi, zhong, ren, none, \n",
      "声调， 2, 3, 2, 4, 4, 4, 5, \n",
      "声母， ch, z, b, sh, zh, r, none, \n",
      "韵母， ong, ai, u, i, ong, en, none, \n",
      "输入法头， ch, z, b, sh, zh, r, none, \n",
      "jie zhi none none none none nian none\n",
      "jie zhi 2 0 1 2 nian ，\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def demo_pinyin():\n",
    "    \"\"\" 汉字转拼音\n",
    "\n",
    "    >>> demo_pinyin()\n",
    "    原文， 重载不是重任！\n",
    "    拼音（数字音调）， [chong2, zai3, bu2, shi4, zhong4, ren4, none5]\n",
    "    拼音（符号音调）， chóng, zǎi, bú, shì, zhòng, rèn, none,\n",
    "    拼音（无音调）， chong, zai, bu, shi, zhong, ren, none,\n",
    "    声调， 2, 3, 2, 4, 4, 4, 5,\n",
    "    声母， ch, z, b, sh, zh, r, none,\n",
    "    韵母， ong, ai, u, i, ong, en, none,\n",
    "    输入法头， ch, z, b, sh, zh, r, none,\n",
    "    jie zhi none none none none nian none\n",
    "    jie zhi 2 0 1 2 nian ，\n",
    "    \"\"\"\n",
    "    Pinyin = JClass(\"com.hankcs.hanlp.dictionary.py.Pinyin\")\n",
    "    text = \"重载不是重任！\"\n",
    "    pinyin_list = HanLP.convertToPinyinList(text)\n",
    "\n",
    "    print(\"原文，\", end=\" \")\n",
    "    print(text)\n",
    "    print(\"拼音（数字音调），\", end=\" \")\n",
    "    print(pinyin_list)\n",
    "    print(\"拼音（符号音调），\", end=\" \")\n",
    "    for pinyin in pinyin_list:\n",
    "        print(\"%s,\" % pinyin.getPinyinWithToneMark(), end=\" \")\n",
    "    print(\"\\n拼音（无音调），\", end=\" \")\n",
    "    for pinyin in pinyin_list:\n",
    "        print(\"%s,\" % pinyin.getPinyinWithoutTone(), end=\" \")\n",
    "    print(\"\\n声调，\", end=\" \")\n",
    "    for pinyin in pinyin_list:\n",
    "        print(\"%s,\" % pinyin.getTone(), end=\" \")\n",
    "    print(\"\\n声母，\", end=\" \")\n",
    "    for pinyin in pinyin_list:\n",
    "        print(\"%s,\" % pinyin.getShengmu(), end=\" \")\n",
    "    print(\"\\n韵母，\", end=\" \")\n",
    "    for pinyin in pinyin_list:\n",
    "        print(\"%s,\" % pinyin.getYunmu(), end=\" \")\n",
    "    print(\"\\n输入法头，\", end=\" \")\n",
    "    for pinyin in pinyin_list:\n",
    "        print(\"%s,\" % pinyin.getHead(), end=\" \")\n",
    "\n",
    "    print()\n",
    "    # 拼音转换可选保留无拼音的原字符\n",
    "    print(HanLP.convertToPinyinString(\"截至2012年，\", \" \", True))\n",
    "    print(HanLP.convertToPinyinString(\"截至2012年，\", \" \", False))\n",
    "    \n",
    "demo_pinyin()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T15:28:43.742446Z",
     "start_time": "2018-11-06T15:28:43.731067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chóng zǎi bú shì zhòng rèn\n"
     ]
    }
   ],
   "source": [
    "sentence=\"重载不是重任\"\n",
    "Pinyin = JClass(\"com.hankcs.hanlp.dictionary.py.Pinyin\")\n",
    "pinyin_list = HanLP.convertToPinyinList(sentence)\n",
    "l=[]\n",
    "for pinyin in pinyin_list:\n",
    "    l.append(\"%s\" % pinyin.getPinyinWithToneMark())\n",
    "print(\" \".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:53:44.751817Z",
     "start_time": "2018-11-06T13:53:44.718581Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_pinyin_to_chinese():\n",
    "    \"\"\" HanLP中的数据结构和接口是灵活的，组合这些接口，可以自己创造新功能\n",
    "\n",
    "    >>> demo_pinyin_to_chinese()\n",
    "    [renmenrenweiyalujiangbujian/null, lvse/[滤色, 绿色]]\n",
    "    \"\"\"\n",
    "    AhoCorasickDoubleArrayTrie = JClass(\n",
    "        \"com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie\")\n",
    "    StringDictionary = JClass(\n",
    "        \"com.hankcs.hanlp.corpus.dictionary.StringDictionary\")\n",
    "    CommonAhoCorasickDoubleArrayTrieSegment = JClass(\n",
    "        \"com.hankcs.hanlp.seg.Other.CommonAhoCorasickDoubleArrayTrieSegment\")\n",
    "    CommonAhoCorasickSegmentUtil = JClass(\n",
    "        \"com.hankcs.hanlp.seg.Other.CommonAhoCorasickSegmentUtil\")\n",
    "    Config = JClass(\"com.hankcs.hanlp.HanLP$Config\")\n",
    "\n",
    "    TreeMap = JClass(\"java.util.TreeMap\")\n",
    "    TreeSet = JClass(\"java.util.TreeSet\")\n",
    "\n",
    "    dictionary = StringDictionary()\n",
    "    dictionary.load(Config.PinyinDictionaryPath)\n",
    "    entry = {}\n",
    "    m_map = TreeMap()\n",
    "    for entry in dictionary.entrySet():\n",
    "        pinyins = entry.getValue().replace(\"[\\\\d,]\", \"\")\n",
    "        words = m_map.get(pinyins)\n",
    "        if words is None:\n",
    "            words = TreeSet()\n",
    "            m_map.put(pinyins, words)\n",
    "        words.add(entry.getKey())\n",
    "    words = TreeSet()\n",
    "    words.add(\"绿色\")\n",
    "    words.add(\"滤色\")\n",
    "    m_map.put(\"lvse\", words)\n",
    "\n",
    "    segment = CommonAhoCorasickDoubleArrayTrieSegment(m_map)\n",
    "    print(segment.segment(\"renmenrenweiyalujiangbujianlvse\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:53:49.583645Z",
     "start_time": "2018-11-06T13:53:46.879179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[renmenrenweiyalujiangbujian/null, lvse/[滤色, 绿色]]\n"
     ]
    }
   ],
   "source": [
    "demo_pinyin_to_chinese()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:55:08.373739Z",
     "start_time": "2018-11-06T13:55:08.338942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hanlp, 的, 项目, 地址, 是, https://github.com/hankcs/HanLP, ,,  , 发布, 地址, 是, https://github.com/hankcs/HanLP/releases, ,,  , 我, 有时候, 会, 在, www.hankcs.com, 上面, 发布, 一些, 消息, ,,  , 我, 的, 微博, 是, http://weibo.com/hankcs/, ,, 会, 同步, 推送, hankcs.com, 的, 新闻, 。,  , 听说, ., 中国, 域名, 开放, 申请, 了, ,, 但, 我, 并, 没有, 申请, hankcs.中国, ,, 因为, 穷, ……,  ]\n",
      "https://github.com/hankcs/HanLP\n",
      "https://github.com/hankcs/HanLP/releases\n",
      "www.hankcs.com\n",
      "http://weibo.com/hankcs/\n",
      "hankcs.com\n",
      "hankcs.中国\n"
     ]
    }
   ],
   "source": [
    "def demo_URL_recognition(text):\n",
    "    \"\"\" 演示URL识别\n",
    "\n",
    "    >>> text = '''HanLP的项目地址是https://github.com/hankcs/HanLP，\n",
    "    ... 发布地址是https://github.com/hankcs/HanLP/releases，\n",
    "    ... 我有时候会在www.hankcs.com上面发布一些消息，\n",
    "    ... 我的微博是http://weibo.com/hankcs/，会同步推送hankcs.com的新闻。\n",
    "    ... 听说.中国域名开放申请了,但我并没有申请hankcs.中国,因为穷……\n",
    "    ... '''\n",
    "    >>> demo_URL_recognition(text)\n",
    "    [HanLP/nx, 的/ude1, 项目/n, 地址/n, 是/vshi, https://github.com/hankcs/HanLP/xu, ，/w,\n",
    "    /w, 发布/v, 地址/n, 是/vshi, https://github.com/hankcs/HanLP/releases/xu, ，/w,\n",
    "    /w, 我/rr, 有时候/d, 会/v, 在/p, www.hankcs.com/xu, 上面/f, 发布/v, 一些/m, 消息/n, ，/w,\n",
    "    /w, 我/rr, 的/ude1, 微博/n, 是/vshi, http://weibo.com/hankcs//xu, ，/w, 会/v,\n",
    "        同步/vd, 推送/nz, hankcs.com/xu, 的/ude1, 新闻/n, 。/w,\n",
    "    /w, 听说/v, ./w, 中国/ns, 域名/n, 开放/v, 申请/v, 了/ule, ,/w, 但/c, 我/rr, 并/cc,\n",
    "        没有/v, 申请/v, hankcs.中国/xu, ,/w, 因为/c, 穷/a, ……/w,\n",
    "    /w]\n",
    "    https://github.com/hankcs/HanLP\n",
    "    https://github.com/hankcs/HanLP/releases\n",
    "    www.hankcs.com\n",
    "    http://weibo.com/hankcs/\n",
    "    hankcs.com\n",
    "    hankcs.中国\n",
    "    \"\"\"\n",
    "    Nature = JClass(\"com.hankcs.hanlp.corpus.tag.Nature\")\n",
    "    Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "    URLTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.URLTokenizer\")\n",
    "\n",
    "    term_list = URLTokenizer.segment(text)\n",
    "    print(term_list)\n",
    "    for term in term_list:\n",
    "        if term.nature == Nature.xu:\n",
    "            print(term.word)\n",
    "            \n",
    "text = '''HanLP的项目地址是https://github.com/hankcs/HanLP，\n",
    "发布地址是https://github.com/hankcs/HanLP/releases，\n",
    "我有时候会在www.hankcs.com上面发布一些消息，\n",
    "我的微博是http://weibo.com/hankcs/，会同步推送hankcs.com的新闻。\n",
    "听说.中国域名开放申请了,但我并没有申请hankcs.中国,因为穷……\n",
    "'''\n",
    "demo_URL_recognition(text)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:56:15.767184Z",
     "start_time": "2018-11-06T13:56:15.743648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[北川景子, 参演, 了, 林诣彬, 导演, 的, 《, 速度, 与, 激情, 3, 》]\n",
      "[林志玲, 亮相, 网友, :, 确定, 不是, 波多野结衣, ?]\n",
      "[龟山千广, 和, 近藤公园, 在, 龟山, 公园, 里, 喝酒, 赏花]\n"
     ]
    }
   ],
   "source": [
    "def demo_japanese_name_recognition(sentences):\n",
    "    \"\"\" 日本人名识别\n",
    "\n",
    "    >>> sentences =[\n",
    "    ...    \"北川景子参演了林诣彬导演的《速度与激情3》\",\n",
    "    ...    \"林志玲亮相网友:确定不是波多野结衣？\",\n",
    "    ...    \"龟山千广和近藤公园在龟山公园里喝酒赏花\",\n",
    "    ... ]\n",
    "    >>> demo_japanese_name_recognition(sentences)\n",
    "    [北川景子/nrj, 参演/v, 了/ule, 林诣彬/nr, 导演/nnt, 的/ude1, 《/w, 速度/n, 与/cc, 激情/n, 3/m, 》/w]\n",
    "    [林志玲/nr, 亮相/vi, 网友/n, :/w, 确定/v, 不是/c, 波多野结衣/nrj, ？/w]\n",
    "    [龟山千广/nrj, 和/cc, 近藤公园/nrj, 在/p, 龟山/nz, 公园/n, 里/f, 喝酒/vi, 赏花/nz]\n",
    "    \"\"\"\n",
    "    Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "    Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "\n",
    "    segment = HanLP.newSegment().enableJapaneseNameRecognize(True)\n",
    "    for sentence in sentences:\n",
    "        term_list = segment.seg(sentence)\n",
    "        print(term_list)\n",
    "        \n",
    "sentences =[\n",
    "   \"北川景子参演了林诣彬导演的《速度与激情3》\",\n",
    "   \"林志玲亮相网友:确定不是波多野结衣？\",\n",
    "   \"龟山千广和近藤公园在龟山公园里喝酒赏花\",\n",
    "]\n",
    "demo_japanese_name_recognition(sentences)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:57:08.277755Z",
     "start_time": "2018-11-06T13:57:07.841250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[威廉王子发表演说 呼吁保护野生动物, 英报告说空气污染带来“公共健康危机”]\n",
      "[英报告说空气污染带来“公共健康危机”]\n",
      "[《时代》年度人物最终入围名单出炉 普京马云入选]\n",
      "[魅惑天后许佳慧不爱“预谋” 独唱《许某某》]\n"
     ]
    }
   ],
   "source": [
    "def demo_suggester():\n",
    "    \"\"\" 文本推荐(句子级别，从一系列句子中挑出与输入句子最相似的那一个)\n",
    "\n",
    "    >>> demo_suggester()\n",
    "    [威廉王子发表演说 呼吁保护野生动物, 英报告说空气污染带来“公共健康危机”]\n",
    "    [英报告说空气污染带来“公共健康危机”]\n",
    "    [《时代》年度人物最终入围名单出炉 普京马云入选]\n",
    "    [魅惑天后许佳慧不爱“预谋” 独唱《许某某》]\n",
    "    \"\"\"\n",
    "    Suggester = JClass(\"com.hankcs.hanlp.suggest.Suggester\")\n",
    "    suggester = Suggester()\n",
    "    title_array = [\n",
    "        \"威廉王子发表演说 呼吁保护野生动物\",\n",
    "        \"魅惑天后许佳慧不爱“预谋” 独唱《许某某》\",\n",
    "        \"《时代》年度人物最终入围名单出炉 普京马云入选\",\n",
    "        \"“黑格比”横扫菲：菲吸取“海燕”经验及早疏散\",\n",
    "        \"日本保密法将正式生效 日媒指其损害国民知情权\",\n",
    "        \"英报告说空气污染带来“公共健康危机”\"\n",
    "    ]\n",
    "    for title in title_array:\n",
    "        suggester.addSentence(title)\n",
    "\n",
    "    print(suggester.suggest(\"陈述\", 2))      # 语义\n",
    "    print(suggester.suggest(\"危机公关\", 1))  # 字符\n",
    "    print(suggester.suggest(\"mayun\", 1))   # 拼音\n",
    "    print(suggester.suggest(\"徐家汇\", 1)) # 拼音\n",
    "\n",
    "demo_suggester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T15:33:15.766626Z",
     "start_time": "2018-11-06T15:33:15.592669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「以後等你當上皇后，就能買草莓慶祝了」。發現一根白頭髮\n",
      "凭借笔记本电脑写程序HanLP\n",
      "hankcs在臺灣寫程式碼\n",
      "hankcs在台湾写代码\n",
      "hankcs在香港寫代碼\n",
      "hankcs在香港写代码\n",
      "hankcs在臺灣寫程式碼\n",
      "hankcs在香港寫代碼\n",
      "hankcs在臺灣寫程式碼\n",
      "hankcs在台灣寫代碼\n",
      "hankcs在臺灣寫代碼\n",
      "hankcs在臺灣寫代碼\n"
     ]
    }
   ],
   "source": [
    "def demo_traditional_chinese2simplified_chinese():\n",
    "    \"\"\" 将简繁转换做到极致\n",
    "\n",
    "    >>> demo_traditional_chinese2simplified_chinese()\n",
    "    「以後等你當上皇后，就能買草莓慶祝了」。發現一根白頭髮\n",
    "    凭借笔记本电脑写程序HanLP\n",
    "    hankcs在臺灣寫程式碼\n",
    "    hankcs在台湾写代码\n",
    "    hankcs在香港寫代碼\n",
    "    hankcs在香港写代码\n",
    "    hankcs在臺灣寫程式碼\n",
    "    hankcs在香港寫代碼\n",
    "    hankcs在臺灣寫程式碼\n",
    "    hankcs在台灣寫代碼\n",
    "    hankcs在臺灣寫代碼\n",
    "    hankcs在臺灣寫代碼\n",
    "    \"\"\"\n",
    "    print(HanLP.convertToTraditionalChinese(\"“以后等你当上皇后，就能买草莓庆祝了”。发现一根白头发\"))\n",
    "    print(HanLP.convertToSimplifiedChinese(\"憑藉筆記簿型電腦寫程式HanLP\"))\n",
    "    # 简体转台湾繁体\n",
    "    print(HanLP.s2tw(\"hankcs在台湾写代码\"))\n",
    "    # 台湾繁体转简体\n",
    "    print(HanLP.tw2s(\"hankcs在臺灣寫程式碼\"))\n",
    "    # 简体转香港繁体\n",
    "    print(HanLP.s2hk(\"hankcs在香港写代码\"))\n",
    "    # 香港繁体转简体\n",
    "    print(HanLP.hk2s(\"hankcs在香港寫代碼\"))\n",
    "    # 香港繁体转台湾繁体\n",
    "    print(HanLP.hk2tw(\"hankcs在臺灣寫代碼\"))\n",
    "    # 台湾繁体转香港繁体\n",
    "    print(HanLP.tw2hk(\"hankcs在香港寫程式碼\"))\n",
    "\n",
    "    # 香港/台湾繁体和HanLP标准繁体的互转\n",
    "    print(HanLP.t2tw(\"hankcs在臺灣寫代碼\"))\n",
    "    print(HanLP.t2hk(\"hankcs在臺灣寫代碼\"))\n",
    "\n",
    "    print(HanLP.tw2t(\"hankcs在臺灣寫程式碼\"))\n",
    "    print(HanLP.hk2t(\"hankcs在台灣寫代碼\"))\n",
    "\n",
    "demo_traditional_chinese2simplified_chinese()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
